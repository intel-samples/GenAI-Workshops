{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1c052f-2263-4b74-986d-66ab48ff4a24",
   "metadata": {},
   "source": [
    "<img src=\"https://opea.dev/wp-content/uploads/sites/9/2024/04/opea-horizontal-color.svg\" alt=\"OPEA Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2afbf3-403d-49ec-8aad-e7d2e3932535",
   "metadata": {},
   "source": [
    "# Deploy and Learn ChatQnA using OPEA on Intel Tiber AI Cloud \n",
    "## Replace your LLM model\n",
    "\n",
    "To deploy the `chatqna` application with a custom Hugging Face model (e.g., `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`), run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!helm upgrade --install chatqna oci://ghcr.io/opea-project/charts/chatqna \\\n",
    "  --set vllm.LLM_MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "  -f cpu-values.yaml && kubectl logs -l app=chatqna --tail=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205891a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154870c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49c4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fec4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24a1468",
   "metadata": {},
   "source": [
    "## 1. Update config map \n",
    "\n",
    "ConfigMaps are used to store configuration settings that your applications need to run. These settings can be environment variables, command-line arguments, or configuration files. By using ConfigMaps, you can update your application's configuration without rebuilding or redeploying your containers.\n",
    "\n",
    "✅ Benefits of Using ConfigMaps\n",
    "\n",
    "- Separation of Concerns: Keeps configuration separate from application code.​\n",
    "- Easy Updates: Allows you to update configuration without redeploying your application.​\n",
    "- Environment Specific Configurations: Facilitates different configurations for different environments (e.g., development, staging, production).​\n",
    "- Dynamic Configuration: Enables applications to adapt to configuration changes in real-time.​\n",
    "\n",
    "To list all ConfigMaps in the current namespace:​​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd54b617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                            DATA   AGE\n",
      "chatqna-chatqna-ui-config       4      4d22h\n",
      "chatqna-data-prep-config        20     4d22h\n",
      "chatqna-nginx-config            7      4d22h\n",
      "chatqna-retriever-usvc-config   19     4d22h\n",
      "chatqna-tei-config              11     4d22h\n",
      "chatqna-teirerank-config        11     4d22h\n",
      "chatqna-vllm-config             11     4d22h\n",
      "kube-root-ca.crt                1      25d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get configmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34749c",
   "metadata": {},
   "source": [
    "This command updates the **LLM_MODEL_ID** from `meta-llama/Meta-Llama-3-8B-Instruct` to `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`, a multimodal large language model developed by Alibaba Cloud's Qwen team. ​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8d886b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configmap/chatqna-vllm-config patched (no change)\n"
     ]
    }
   ],
   "source": [
    "!kubectl patch configmap chatqna-vllm-config -p '{\"data\": {\"LLM_MODEL_ID\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1430179",
   "metadata": {},
   "source": [
    "## 2. Update the POD with the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1af566",
   "metadata": {},
   "source": [
    "Check the current deployments\n",
    "Run the following command to list all the deployments in your Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80f2685c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                      READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "chatqna                   1/1     1            1           4d22h\n",
      "chatqna-chatqna-ui        1/1     1            1           4d22h\n",
      "chatqna-data-prep         1/1     1            1           4d22h\n",
      "chatqna-nginx             1/1     1            1           4d22h\n",
      "chatqna-redis-vector-db   1/1     1            1           4d22h\n",
      "chatqna-retriever-usvc    1/1     1            1           4d22h\n",
      "chatqna-tei               1/1     1            1           4d22h\n",
      "chatqna-teirerank         1/1     1            1           4d22h\n",
      "chatqna-vllm              1/1     1            1           4d22h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf05daae",
   "metadata": {},
   "source": [
    "This helps you confirm that the deployment (`chatqna-vllm` in this case) exists and is ready to be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034125d8",
   "metadata": {},
   "source": [
    "Restart the deployment to apply the new model without needing to manually delete pods. This command triggers a rolling restart, meaning Kubernetes will gracefully recreate the pods one by one, minimizing downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46637c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/chatqna-vllm restarted\n"
     ]
    }
   ],
   "source": [
    "!helm upgrade --install chatqna oci://ghcr.io/opea-project/charts/chatqna --set vllm.LLM_MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B -f cpu-values.yaml && kubectl logs -l app=chatqna --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c9cc6",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Restarting the deployment ensures the new model files/configurations are picked up without needing to redeploy the entire application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c24f73",
   "metadata": {},
   "source": [
    "## 3. Check the new pods. \n",
    "\n",
    "After restarting, list all pods to verify the new pods are running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cd7c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                       READY   STATUS    RESTARTS   AGE\n",
      "chatqna-868d98c5bf-vb2bk                   1/1     Running   0          4d22h\n",
      "chatqna-chatqna-ui-ffd74c8d8-j67gr         1/1     Running   0          41m\n",
      "chatqna-data-prep-59849c8885-xn8pq         1/1     Running   0          4d22h\n",
      "chatqna-nginx-6c855d856c-4nds9             1/1     Running   0          4d22h\n",
      "chatqna-redis-vector-db-8566ffdb78-f8d62   1/1     Running   0          4d22h\n",
      "chatqna-retriever-usvc-57c8c4c7d5-9rfrd    1/1     Running   0          4d22h\n",
      "chatqna-tei-9c46456c7-z89lc                1/1     Running   0          4d22h\n",
      "chatqna-teirerank-5d4c49cd8d-sq7jc         1/1     Running   0          4d22h\n",
      "chatqna-vllm-58d8d9cf69-xtgb8              1/1     Running   0          11m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "661b107a-a55c-42ac-a59d-d4801cfffe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vLLM pod: chatqna-vllm-58d8d9cf69-xtgb8\n"
     ]
    }
   ],
   "source": [
    "vllm_pod = !kubectl get pods --no-headers | grep chatqna-vllm | awk '{print $1}'\n",
    "vllm_pod = vllm_pod[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02eb2c0",
   "metadata": {},
   "source": [
    "Look for the new pods related to your deployment (e.g., `chatqna-vllm-xxxx`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf744d8",
   "metadata": {},
   "source": [
    "Check environment variables inside the pod. Verify that the correct model environment variable (LLM_MODEL_ID) is set inside the running pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17542dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulted container \"vllm\" out of: vllm, model-downloader (init)\n",
      "meta-llama/Meta-Llama-3-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Check you replaced the variable\n",
    "!kubectl exec -it {vllm_pod} -- printenv LLM_MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1b3a9",
   "metadata": {},
   "source": [
    "Confirm the model was correctly updated\n",
    "Make sure that the `LLM_MODEL_ID` printed matches the model you intended to deploy.\n",
    "If it doesn’t, double-check your deployment configuration and environment variable settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98fc414-065e-49c1-a96a-34e5490578ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
