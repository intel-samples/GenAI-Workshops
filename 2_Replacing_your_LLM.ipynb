{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce1c052f-2263-4b74-986d-66ab48ff4a24",
   "metadata": {},
   "source": [
    "<img src=\"https://opea.dev/wp-content/uploads/sites/9/2024/04/opea-horizontal-color.svg\" alt=\"OPEA Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca985481",
   "metadata": {},
   "source": [
    "# Deploy and Learn ChatQnA using OPEA on Intel Tiber AI Cloud \n",
    "## Replace your LLM model from your deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891b72b",
   "metadata": {},
   "source": [
    "# üì¶ What Is a Helm Chart?\n",
    "\n",
    "A **Helm chart** is a collection of YAML templates that describe Kubernetes resources. Helm makes it easy to package, configure, and deploy applications to Kubernetes clusters. It is the de facto standard for Kubernetes application deployment.\n",
    "\n",
    "Helm allows developers to:\n",
    "- Define application components using templates\n",
    "- Configure deployments using values files (`values.yaml`)\n",
    "- Deploy applications with a single command\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665ad60",
   "metadata": {},
   "source": [
    "## üì¶ OPEA Helm Chart Structure (OPEA GenAIInfra)\n",
    "\n",
    "HELM charts are part of the [GenAIInfra](https://github.com/opea-project/GenAIInfra) repository, which provides infrastructure templates to deploy multiple OPEA blueprints on Kubernetes.\n",
    "\n",
    "For this example, we will be exploring ChatqnA blueprint. The `chatqna` chart defines the resources required to deploy an end-to-end RAG-based question-answering application using modular microservices (LLM backend, retriever service, data prep, etc.).\n",
    "\n",
    "```text\n",
    "helm-charts/\n",
    "‚îî‚îÄ‚îÄ chatqna/\n",
    "    ‚îú‚îÄ‚îÄ Chart.yaml                  # Chart metadata (e.g., name, version, description)\n",
    "    ‚îú‚îÄ‚îÄ values.yaml                 # Default configuration values for the chart\n",
    "    ‚îú‚îÄ‚îÄ templates/                  # Kubernetes resource templates\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml         # Defines Pods, containers, and other resources for LLM, retriever, etc.\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ service.yaml            # Defines the networking setup and service exposure\n",
    "    ‚îú‚îÄ‚îÄ cpu-values.yaml          # ‚úÖ Optional config file for CPU-only environments\n",
    "    ‚îî‚îÄ‚îÄ ...                      # ‚úÖ Optional config file for multiple configurations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9f7c2",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è How to Customize a Helm Chart. \n",
    "\n",
    "### ‚öôÔ∏è cpu-values.yaml ‚Äî CPU-Optimized Configuration\n",
    "\n",
    "The `cpu-values.yaml` file is a custom override used to configure the `chatqna` Helm chart to run efficiently on CPU-only infrastructure. This is particularly useful for developers working on local machines, CI/CD pipelines, or edge environments.\n",
    "\n",
    "This file overrides default values found in `values.yaml`.\n",
    "\n",
    "### üîç Key Overrides in `cpu-values.yaml`\n",
    "\n",
    "```yaml\n",
    "vllm:\n",
    "  LLM_MODEL_ID: meta-llama/Meta-Llama-3-8B-Instruct\n",
    "  # Uncomment for DeepSeek models:\n",
    "  # VLLM_CPU_KVCACHE_SPACE: 40\n",
    "  # resources:\n",
    "  #   requests:\n",
    "  #     memory: 60Gi\n",
    "## Custom Configuration: `cpu-values.yaml`\n",
    "```\n",
    "\n",
    "The `cpu-values.yaml` file is **part of the Helm chart directory** itself ([`helm-charts/chatqna`](https://github.com/opea-project/GenAIInfra/tree/main/helm-charts/chatqna)), because it is meant to be a **user-provided override configuration**.\n",
    "\n",
    "### Benefits of using specific configuration files.\n",
    "\n",
    "- Keeps the chart generic and reusable.\n",
    "- Allows multiple profiles like:\n",
    "  - `cpu-values.yaml` ‚Äì for CPU-based deployments.\n",
    "  - `cpu-ollama-values.yaml` ‚Äì for CPU-based deployments using Ollama as LLM.\n",
    "  - `cpu-milvus-values.yaml` ‚Äì for CPU-based deployments using Milvus as Vector DB.\n",
    "- Makes upgrades and version tracking easier since your overrides are managed separately.\n",
    "\n",
    "To deploy using `cpu-values.yaml` and to change `chatqna` application with a custom Hugging Face model (e.g., `deepseek-ai/DeepSeek-R1-Distill-Qwen-7B`), run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/devcloud/GenAI-Workshops/GenAIExamples/ChatQnA/kubernetes/helm\n",
    "\n",
    "!helm upgrade --install chatqna oci://ghcr.io/opea-project/charts/chatqna \\\n",
    "  --set global.HUGGINGFACEHUB_API_TOKEN=\"your_huggingface_token\" \\\n",
    "  --set vllm.LLM_MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
    "  --set vllm.VLLM_CPU_KVCACHE_SPACE=20 \\\n",
    "  --set vllm.resources.requests.memory=60Gi \\\n",
    "  -f cpu-values.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d9353",
   "metadata": {},
   "source": [
    "## üöÄ What Happens When You Run This Helm Command\n",
    "\n",
    "This Helm command installs (or upgrades) the `chatqna` application using the OPEA Helm chart hosted in GitHub Container Registry. Here's a breakdown of what happens step-by-step:\n",
    "\n",
    "### 1. üß† Loads the Helm Chart\n",
    "The command pulls the `chatqna` Helm chart from: `oci://ghcr.io/opea-project/charts/chatqna`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c27c9f",
   "metadata": {},
   "source": [
    "\n",
    "This chart includes Kubernetes manifests for deploying all required components such as:\n",
    "- LLM (vLLM-based model serving)\n",
    "- Retriever (e.g. Redis, Qdrant)\n",
    "- Frontend and backend services\n",
    "- ConfigMaps, Services, Deployments\n",
    "\n",
    "### 2. ‚öôÔ∏è Applies Your CPU-Specific Overrides\n",
    "It loads custom settings from your `cpu-values.yaml` file to optimize the deployment for CPU-based environments.\n",
    "\n",
    "Then it overrides/adds more specific values using `--set`:\n",
    "\n",
    "| Setting | Purpose |\n",
    "|--------|---------|\n",
    "| `vllm.LLM_MODEL_ID=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B` | Sets the LLM model to use. This is a compact, efficient model ideal for CPU inference. |\n",
    "| `vllm.VLLM_CPU_KVCACHE_SPACE=40` | Reserves 40GB for vLLM's KV cache (used to speed up attention mechanisms during inference). |\n",
    "| `vllm.resources.requests.memory=60Gi` | Requests 60GB of RAM for the pod running vLLM. This ensures Kubernetes schedules it with enough memory. |\n",
    "\n",
    "### 3. üö¢ Installs or Updates the Release\n",
    "Helm will:\n",
    "- Install the chart as a new release named `chatqna` (if not installed yet)\n",
    "- Or **upgrade** the existing `chatqna` release with the new settings\n",
    "\n",
    "### 4. üì° Starts the Pods\n",
    "Kubernetes will:\n",
    "- Pull the necessary Docker images (e.g. vLLM, retriever, UI)\n",
    "- Allocate CPU and memory as specified\n",
    "- Launch the containers\n",
    "- Monitor readiness/liveness probes\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Result: ChatQnA is Live\n",
    "\n",
    "Once complete, you'll have the `chatqna` service running on your cluster with:\n",
    "\n",
    "- A **DeepSeek-based LLM** running via vLLM on CPU\n",
    "- A retriever connected to your vector database (Redis/Qdrant)\n",
    "- Optional frontend and backend endpoints available via service\n",
    "\n",
    "You can now access the application (e.g., via port-forward) or query the model via API.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Useful Follow-up Commands\n",
    "\n",
    "Check pod status:\n",
    "```bash\n",
    "kubectl get pods\n",
    "```\n",
    "## ‚úÖ Verifying That the Correct Model Is Running\n",
    "\n",
    "After deploying your Helm chart, you‚Äôll want to ensure that the expected model is actually being served by the application. Here's how to confirm this:\n",
    "\n",
    "### 1. üìÑ Check the Logs using `k9s` in your console\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
